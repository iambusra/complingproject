{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "the_ultimate_webscraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGeV7uGxk4qSIAfsRateyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iambusra/complingproject/blob/main/the_ultimate_webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W6w7_r2qWtZ"
      },
      "source": [
        "# **Brief Intro**\n",
        "##### **Ne yapacağız?** \n",
        "Google News üzerinden haber çekeceğiz.\n",
        "<br>\n",
        "<br>\n",
        "##### **Nasıl yapacağız?** \n",
        "Requests ile sayfaya req atacağız, BeautifulSoup ile html kodu içinde gezeceğiz, Pandas ile de metinleri, başlıkları ve linkleri Excel dosyasına kaydedeceğiz. Sonra bu Excel dosyasının içinde looplar ile dolaşarak linklere gideceğiz, body metinlerini çekeceğiz ve bu metinleri yine dosyaya kaydedeceğiz. \n",
        "<br>\n",
        "<br>\n",
        "##### **Neden Excel?** \n",
        "Temiz, erişilebilir ve kolay diye böyle 1 karar aldım. Datamızı initially incelerken Excel üzerinden bakmak çok daha kolay olur, kodla uğraşmadan çıplak gözle de görebiliriz, annotation yapmak istersek Google Sheets üzerinden paylaşıp annotation yaparız, ayrı ayrı çektiğimiz dataları kolayca birleştirebiliriz vs. Gerekince/gerekirse metinleri Excel'den alıp txt'ye aktarmak da çok kolay. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEzys0Wrsrp4"
      },
      "source": [
        "## Part 1: Lazım paketler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbVOspxGRchK"
      },
      "source": [
        "**Lazım şeyleri import edelim.** Bunları önce pip ile bilgisayarınıza install etmeniz gerekecek muhtemelen. Pip komutlarını da yazdım, uncomment edip bir kere çalıştırdıktan sonra tekrar comment out ediniz. Sorun olursa lmk. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDNpkZtaRpWZ"
      },
      "source": [
        "#!pip install requests\n",
        "import requests  \n",
        "#html requestleri atabilmemiz için lazım. \n",
        "\n",
        "#!pip install beautifulsoup4\n",
        "from bs4 import BeautifulSoup as bs \n",
        "#actual scraping işleri için lazım\n",
        "\n",
        "#!pip install pandas\n",
        "#!pip install openpyxl\n",
        "#!pip install xlsxwriter\n",
        "#!pip install xlrd\n",
        "#!pip install numpy\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "import xlsxwriter\n",
        "import xlrd\n",
        "import numpy\n",
        "#excel işleri için lazım bu iki paket\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVSm7L2Ls-WQ"
      },
      "source": [
        "## Part 2: Sayfaya kısa bir bakış"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24YK6kkn2tgN"
      },
      "source": [
        "##### **Link meselesi.**\n",
        "Tarih aralığı bölüşürüz demiştim ama Google buna izin vermiyor lol. Son x saat, son x gün, son x hafta ve son x ay şeklinde arayabiliyoruz, spesifik tarih aralığı giremiyoruz. Birkaç gün üst üste parametreyi son 24 saat yaparak scrape edebiliriz. Bunun için günleri paylaşmamız gerekir. Grupta konuşalım."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn4Oq4ThaGdv"
      },
      "source": [
        "##### **Sayfanın html contentine bi bakalım.**\n",
        "Bize başlık + link lazım, bunların html tagleri neymiş ona göz atacağız.\n",
        "<br>\n",
        "<br>\n",
        "not: 100 haber başlığı çekeceğiz, max scroll bu kadar. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZy9UBWsaMJg"
      },
      "source": [
        "r = requests.get (\"https://news.google.com/rss/search?q=kadin+surucu&ceid=TR:tr&hl=tr&gl=TR\")\n",
        "# linkteki sayfanın html içeriğini çeken kod parçası. requests paketini kullandık.\n",
        "\n",
        "soup = bs(r.content)\n",
        "# beautiful soup objectine çevirdik ki o paketi kullanarak html kodu içinde gezebilelim,\n",
        "# istediğimiz kısımları çekebilelim vs.\n",
        "\n",
        "print(soup.prettify())\n",
        "# soup object olarak tanımladığımız şeyi print ediyoruz, bakalım sayfa olması gerektiği\n",
        "# şekilde gelmiş mi. prettify'ı şu sebeple kullanıyoruz, html kodları aslında matruşka gibi \n",
        "# nested structure'lar. direkt bu kodu çektiğimizde indentation gibi nestedness'ı\n",
        "# daha iyi anlamamıza yardımcı olan formattingler kayboluyor. prettify dediğimizde indentation\n",
        "# falan yaparak kodu okuması kolay bir hale getiriyoruz.\n",
        "# 100 haber çektiğimiz için print gigantic olacak. kodu birkaç kere çalıştırmanız gerekirse\n",
        "# kalabalık yapmasın diye bu print kısmını comment out edebilirsiniz, aşağıdaki kodlarda\n",
        "# buraya bi dependency yok.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioNniCoKb1Vi"
      },
      "source": [
        "**Esas scraping kısmı başlıyor.** Yukarıda gördük ki `<title>` 'da haberin başlığı var, `<link/>` 'te ve `<description>` 'da da linkler var. Bizim sayfadan bu kısımları almamız lazım. \n",
        "\n",
        " html'de etiket formatı şöyle:\n",
        "\n",
        "\n",
        "```\n",
        "<etiket> #burada etiket başlıyor.\n",
        "content\n",
        "</etiket> #burada etiket bitiyor.\n",
        "```\n",
        "\n",
        "`<link/>` kısmında etiket temiz halde değil. CSS ve html çok bilmediğim için neden böyle bir stil tercih ettiklerini net açıklayamayacağım, prolly Google'ın kaynak sitelerden veri çekme processiyle ilgili. So, `<link/>` etiketi yamuk olduğu için onun yerine `<description>` 'ı kullanacağız fakat bu da bize bir miktar unnecessary veri getirecek, çok tidy olmayacak. o yüzden ayrıca oradan gelen veriyi temizlememiz gerekecek :')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-OiIos7c5dA"
      },
      "source": [
        "basliklar0 = soup.findAll(\"title\")\n",
        "# burada <title> tagine sahip olan her şeyi bul getir diyoruz. direkt basliklar objesini de\n",
        "# print edebiliriz ama line break filan olsun, okuması kolay olsun istiyoruz, o yüzden\n",
        "# aşağıya minik bi for loop yazdım.\n",
        "\n",
        "for baslik in basliklar0:\n",
        "  print(baslik)\n",
        "# bakalım başlıkları görebiliyor muyuz. yis, görebiliyoruz. \n",
        "# kaosu önlemek ve çalışma alanımızı temiz tutmak adına baktıktan sonra böyle massive\n",
        "# şeyler print eden line'ları comment out edebiliriz, hatta edelim çünkü 100 satır\n",
        "# scroll etmek zor bir şey."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozQbGTvxg4_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2bc2fa-0fa6-4159-f597-0ab686ee7446"
      },
      "source": [
        "linktag = soup.findAll(\"link\")\n",
        "print(linktag)\n",
        "# şimdi bir küçük <link> etiketine bakalım, lets see how bad it sux.\n",
        "# ohgod bayaa kötü, bize asla content vermedi lol. \"link/\" olarak da denedim, still bad.\n",
        "# yola <description> ile devam edeceiz meçbur."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>, <link/>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBXmIkEbhAFh"
      },
      "source": [
        "linkler0 = soup.findAll(\"description\")\n",
        "for link in linkler0:\n",
        "  print(link)\n",
        "# bu da epey messy bir şeyler print edecek ama we will fix that."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgKsLRuIhNYe"
      },
      "source": [
        "Artık elimizde başlıklar & haber linkleri var. Yay!!! Şimdi bunları bi excelde buluşturacağız. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNR1Ou1UENDF"
      },
      "source": [
        "## Part 3: Datamızı toparlıyoruz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNP_Hzp22sQ"
      },
      "source": [
        "##### **Başlıkları ve haber linklerini birleştirelim**\n",
        "Şimdi başlıklara ve linklere ayrı ayrı eriştik, ama istiyoruz ki şık bi şekilde görelim bunları. Mesela başlık yazsın, altında da o habere giden link olsun gibi. Önce başlıklar ve linklerden oluşan bir dataframe oluşturacağız, sonra da bu dataframei bir excel dosyasına kaydedeceğiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4BplABpEH4d"
      },
      "source": [
        "**Önce dataframe oluşturuyoruz.**\n",
        "Elimizde iki tane liste vardı: links0 ve basliklar0. Bunlarla çalışacağız."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcx6oQIw28gU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a8cdec-043e-4a94-a6ba-c74bcb4539e7"
      },
      "source": [
        "dataframe0 = pd.DataFrame(list(zip(basliklar0, linkler0)), \n",
        "# pd bizi pandas'ın içine götürüyor. buradan DataFrame fonksiyonunu alıyoruz.\n",
        "# liste haline getirmek için list() kullanıyoruz. iki elemanı paralel iterate etmek \n",
        "# için zip() kullanıyoruz. sütunlarımız bunlar olacak.\n",
        "             columns = [\"Haber Başlıkları\", \"Linkler\"])\n",
        "              #sütunlara isim koyuyoruz.\n",
        "\n",
        "print(dataframe0)\n",
        "# isnt she pretty :3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                      Haber Başlıkları                                            Linkler\n",
            "0                   [\"kadin surucu\" - Google Haberler]                                  [Google Haberler]\n",
            "1    [Akasya Durağı'ndan etkilenen kadın sürücü tak...  [<a href=\"https://www.yenisafak.com/hayat/akas...\n",
            "2    [Kısıtlama gününde başka bir aracın sıkıştırdı...  [<a href=\"https://www.imaret.com.tr/kisitlama-...\n",
            "3    [Balıkesir'in Edremit ilçesinde kadın sürücü t...  [<a href=\"https://www.kamu3.com/balikesirin-ed...\n",
            "4    [Bursa'da kadın sürücü çalınan aracının peşind...  [<a href=\"https://www.bgazete.com.tr/haber/691...\n",
            "..                                                 ...                                                ...\n",
            "96   [Kadın sürücü ve otizmli oğlunu trafikte sıkış...  [<a href=\"http://www.gazetevatan.com/kadin-sur...\n",
            "97   [İBB tren sürücüsü işe alacak! İşte başvuru şa...  [<a href=\"https://www.sozcu.com.tr/2019/ekonom...\n",
            "98   [Dalaman – Fethiye arasındaki kazada kadın sür...  [<a href=\"https://www.denizli24haber.com/haber...\n",
            "99           [Kadın sürücü Ayşegül’ü ezdi - Yeni Asır]  [<a href=\"https://www.yeniasir.com.tr/yasam/20...\n",
            "100  [Konya'da kadın sürücü, trafikte tartıştığı er...  [<a href=\"https://www.haberturk.com/konya-da-k...\n",
            "\n",
            "[101 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULQLjMfTJtlc"
      },
      "source": [
        "RegEx ile linkleri sanırım temizleyebiliriz ama çok iyi RegEx bilmediğim için imma skip that part, excel'den elle temizleyeceğim. RegEx'ine güvenen varsa http:// ile başlayan kısmı address edecek bir şey yazabilir mesela :> O regex ile kısa bi Pandas kodu yazıp Linkler kolonunu tertemiz yapabiliriz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwnZa47oKYip"
      },
      "source": [
        "**Şimdi dataframe'imizden excel dosyası oluşturacağız.**\n",
        "<br>\n",
        "Ben bilgisayarımda bir excel dosyası oluşturdum, ismini de DataPart1.xlsx koydum. İçi boş, şimdi onu açıp içini dolduracağız."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHHlH4EKdNq",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f9e4d4b6-f445-4adf-a957-9f8b579b2d12"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#xlsx dosyasını upload ettik."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1473beb1-575b-4c57-811f-4b5b992884e7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1473beb1-575b-4c57-811f-4b5b992884e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving DataPart1.xlsx to DataPart1 (1).xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPX9tc6HPvd4"
      },
      "source": [
        "writer = pd.ExcelWriter('DataPart1.xlsx')\n",
        "# dedik ki Panda, ExcelWriter modülü ile bizim bu dosyayı açsın; bu operasyonun ismi\n",
        "# de writer olsun.\n",
        "\n",
        "dataframe0.to_excel(writer)\n",
        "# dataframe0'daki her şeyi excel dosyamıza yazıyoruz.\n",
        "writer.save()\n",
        "# ve kaydediyoruz. Ben bu dosyayı download edip excelde find & replace ve biraz\n",
        "# da gelişmiş annotation kaslarımla linkleri temizledim. RegEx yazma işini konuşalım,\n",
        "# her dataset için böyle zaman harcamayalım."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUMeiEswVt4v"
      },
      "source": [
        "# Kapanış\n",
        "Bu dosyada oluşturduğumuz Excel'i yeni bir notebook'ta açacağız. Özetle şunları yapacağız: \n",
        "* Python dosyasını yeni notebookta açacağız. Bir dataframe'e dönüştüreceğiz.\n",
        "* Linkler kolonundaki linkleri tek tek alan, o sayfaları ziyaret eden, sayfalardaki `<body>` tagli metinleri çeken ve bu metinleri dataframe'e yapıştıran bir fonksiyon yazacağız. \n",
        "* Yukarıdaki fonksiyonu çalıştırıp yeni dataframe'i excel dosyamıza kaydeceğiz. Böylece elimizde haber başlığı, haber linki ve haber metninden oluşan 4 tane excel dosyası olacak. Bu dosyaları birleştirip sentiment analysis kısmını yaparız sonra da.\n",
        "<br>\n",
        "<br>\n",
        "Şu an beynim yetmiyor ama bu bahsettiğim fonksiyon için yukarıda BeautifulSoup ile yaptığımız, findAll kullandığımız şeyleri bi loop içinde yazacağız. Would you like to give it a try? Ben de biraz dinlenince bakacağım tekrar.\n",
        "<br>\n",
        "<br> \n",
        "Umarım yeterince self explanatory yazmışımdır kodu ve buraya kadar bugsız errorsız gelebilmişsinizdir ✨\n",
        "\n"
      ]
    }
  ]
}